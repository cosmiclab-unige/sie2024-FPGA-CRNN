{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 09:44:43.972498: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-13 09:44:43.997412: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-13 09:44:43.997438: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-13 09:44:43.998120: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-13 09:44:44.002500: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-13 09:44:44.712120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to import handlers from reshape.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from convolution.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from pooling.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from core.py: No module named 'torch'.\n",
      "WARNING: Failed to import handlers from merge.py: No module named 'torch'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rikpi/.local/lib/python3.11/site-packages/hls4ml/converters/__init__.py:27: UserWarning: WARNING: Pytorch converter is not enabled!\n",
      "  warnings.warn(\"WARNING: Pytorch converter is not enabled!\", stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.15.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"PATH\"] = \"=/usr/local/cuda-12.2/bin:$PATH\"\n",
    "#os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH\"\n",
    "#os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-12.2\"\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models, activations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import hls4ml\n",
    "from tqdm import tqdm\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set sample number\n",
    "sample_num = 300\n",
    "\n",
    "dataset_dict={300:'./dataset.npy', 500:'./dataset_500.npy', 800:'./dataset_800.npy', 1000:'./dataset_1000.npy'}\n",
    "\n",
    "\n",
    "# Load the data\n",
    "with open (dataset_dict[sample_num],'rb') as f:\n",
    "  X_train = np.load(f,allow_pickle=True)\n",
    "  Y_train = np.load(f,allow_pickle=True)\n",
    "  X_val = np.load(f,allow_pickle=True)\n",
    "  Y_val = np.load(f,allow_pickle=True)\n",
    "  X_test =np.load(f,allow_pickle=True)\n",
    "  Y_test =np.load(f,allow_pickle=True)\n",
    "\n",
    "num_classes = len(np.unique(Y_train))\n",
    "y_train_onehot = tf.keras.utils.to_categorical(Y_train, num_classes=num_classes)\n",
    "y_val_onehot = tf.keras.utils.to_categorical(Y_val, num_classes=num_classes)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(Y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOHAMAD 1DCNN CLASS\n",
    "class CNN_1D():\n",
    "    \"\"\"\n",
    "    CNN_1D: Class to train a 1D Convolutional Neural Network\n",
    "    Attributes:\n",
    "    filters: List of filters for the convolutional layers\n",
    "    kernels: List of kernel sizes for the convolutional layers\n",
    "    window: Window size for the input data\n",
    "    n_folds: Number of folds for cross-validation\n",
    "    channels: Number of channels in the input data\n",
    "    batch_size: Batch size for training\n",
    "    epochs: Number of epochs for training\n",
    "    learning_rate: Learning rate for the optimizer\n",
    "    optimizer: Optimizer for training\n",
    "    loss: Loss function for training\n",
    "    metrics: Metrics for training\n",
    "    callbacks: Callbacks for training\n",
    "    norm_dict: Dictionary with normalization parameters\n",
    "    train: Method to train the model\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 X_train,\n",
    "                 y_train,\n",
    "                 X_val,\n",
    "                 y_val,\n",
    "                 X_test,\n",
    "                 y_test):\n",
    "        self.filters = [[8], [16],[32],[64]]\n",
    "        self.kernels = [2, 3, 4, 5]\n",
    "        self.batch_size = 64\n",
    "        self.epochs = 300\n",
    "        self.learning_rate = 0.001\n",
    "        self.n_classes = 8\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        self.loss = 'categorical_crossentropy'\n",
    "        self.metrics = ['accuracy']\n",
    "        self.callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
    "                         # tf.keras.callbacks.LearningRateScheduler(schedule=lambda epoch, lr: lr * 0.5 if epoch % 5 == 0 and epoch != 0 and lr>1e-4 else lr)]\n",
    "\n",
    "        # Assign the training, validation, and testing data\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        ########## Train ################\n",
    "        self.train()\n",
    "\n",
    "    def __create_model(self, filters, kernel, n_classes, verbose=1):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        for i, f in enumerate(filters):\n",
    "            if i == 0:\n",
    "                model.add(tf.keras.layers.Conv1D(f, kernel, input_shape=(self.X_train.shape[1:]), padding='same'))\n",
    "                #model.add(tf.keras.layers.BatchNormalization())\n",
    "                model.add(tf.keras.layers.Activation('relu'))\n",
    "            else:\n",
    "                model.add(tf.keras.layers.Conv1D(f, kernel, activation='relu', padding='same'))\n",
    "                #model.add(tf.keras.layers.BatchNormalization())\n",
    "                model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "            model.add(tf.keras.layers.MaxPooling1D(2))\n",
    "        # model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "        # model.add(tf.keras.layers.AveragePooling1D(pool_size=150))\n",
    "        model.add(layers.Flatten())\n",
    "        #model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "        if verbose:\n",
    "            model.summary()\n",
    "        return model\n",
    "    def train(self):\n",
    "        best_val_acc = 0\n",
    "        best_f=None\n",
    "        best_k= None\n",
    "        initial_optimizer_config = self.optimizer.get_config()\n",
    "        with tqdm(total=len(self.filters) * len(self.kernels), desc=\"Training Progress\", unit=\"iteration\") as pbar:\n",
    "            for f in self.filters:\n",
    "                for k in self.kernels:\n",
    "                    model = self.__create_model(f, k, self.n_classes, verbose=0)\n",
    "                    optimizer = type(self.optimizer).from_config(initial_optimizer_config)\n",
    "                    model.compile(optimizer=optimizer, loss=self.loss, metrics=self.metrics)\n",
    "                    history = model.fit(self.X_train,self.y_train, epochs=self.epochs, batch_size=self.batch_size, validation_data=(self.X_val, self.y_val),callbacks=self.callbacks, verbose=0)\n",
    "                    if history.history['val_accuracy'][-1] > best_val_acc:\n",
    "                        # print('best val acc:', history.history['val_accuracy'][-1])\n",
    "                        best_val_acc = history.history['val_accuracy'][-1]\n",
    "                        model.save(f\"model_{f}_{k}.h5\")\n",
    "                        best_f = f\n",
    "                        best_k = k\n",
    "                    pbar.update(1)\n",
    "        best_model = tf.keras.models.load_model(f\"model_{best_f}_{best_k}.h5\")\n",
    "        # best_model.compile(optimizer=self.optimizer, loss=self.loss, metrics=self.metrics)\n",
    "        _, train_accuracy = best_model.evaluate(self.X_train,self.y_train, verbose=0)\n",
    "        _, test_accuracy = best_model.evaluate(self.X_test,self.y_test, verbose=0)\n",
    "        _, val_accuracy = best_model.evaluate(self.X_val,self.y_val, verbose=0)\n",
    "        print(f\"best_f: {best_f},best_k: {best_k}\")\n",
    "        print(\"Best model\")\n",
    "        best_model.summary()\n",
    "        print(\"Train_accuracy:\",train_accuracy,\"Val accuracy:\", val_accuracy, \" Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 16/16 [10:28<00:00, 39.29s/iteration]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_f: [64],best_k: 5\n",
      "Best model\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_51 (Conv1D)          (None, 300, 64)           2624      \n",
      "                                                                 \n",
      " activation_51 (Activation)  (None, 300, 64)           0         \n",
      "                                                                 \n",
      " max_pooling1d_51 (MaxPooli  (None, 150, 64)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " flatten_47 (Flatten)        (None, 9600)              0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 8)                 76808     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79432 (310.28 KB)\n",
      "Trainable params: 79432 (310.28 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Train_accuracy: 0.9544695019721985 Val accuracy: 0.849418580532074  Test Accuracy: 0.8462209105491638\n"
     ]
    }
   ],
   "source": [
    "# TRAIN 1DCNN CLASS\n",
    "model = CNN_1D(X_train, y_train_onehot, X_val, y_val_onehot, X_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_59 (Conv1D)          (None, 300, 32)           2080      \n",
      "                                                                 \n",
      " batch_normalization_37 (Ba  (None, 300, 32)           128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 12)                2160      \n",
      "                                                                 \n",
      " flatten_54 (Flatten)        (None, 12)                0         \n",
      "                                                                 \n",
      " output_dense (Dense)        (None, 8)                 104       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4472 (17.47 KB)\n",
      "Trainable params: 4408 (17.22 KB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (sample_num,8)\n",
    "num_classes = 8\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=input_shape))\n",
    "model.add(tf.keras.layers.Conv1D(32,8, activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.LSTM(12))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(num_classes, activation='softmax', name='output_dense'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "215/215 [==============================] - 11s 27ms/step - loss: 1.8734 - accuracy: 0.2582 - val_loss: 1.7180 - val_accuracy: 0.3093\n",
      "Epoch 2/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 1.5907 - accuracy: 0.3493 - val_loss: 1.5929 - val_accuracy: 0.3433\n",
      "Epoch 3/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 1.5773 - accuracy: 0.3546 - val_loss: 1.7435 - val_accuracy: 0.3174\n",
      "Epoch 4/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 1.4664 - accuracy: 0.4011 - val_loss: 1.3082 - val_accuracy: 0.4529\n",
      "Epoch 5/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 1.4209 - accuracy: 0.4350 - val_loss: 1.3139 - val_accuracy: 0.4529\n",
      "Epoch 6/200\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 1.2097 - accuracy: 0.5126 - val_loss: 1.1778 - val_accuracy: 0.5273\n",
      "Epoch 7/200\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 1.1026 - accuracy: 0.5579 - val_loss: 1.0103 - val_accuracy: 0.5910\n",
      "Epoch 8/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.9593 - accuracy: 0.6209 - val_loss: 0.8910 - val_accuracy: 0.6448\n",
      "Epoch 9/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.8899 - accuracy: 0.6459 - val_loss: 0.8548 - val_accuracy: 0.6430\n",
      "Epoch 10/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.8535 - accuracy: 0.6542 - val_loss: 0.8170 - val_accuracy: 0.6744\n",
      "Epoch 11/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.8169 - accuracy: 0.6707 - val_loss: 0.8157 - val_accuracy: 0.6622\n",
      "Epoch 12/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.7380 - accuracy: 0.6986 - val_loss: 0.7130 - val_accuracy: 0.6988\n",
      "Epoch 13/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.7121 - accuracy: 0.7140 - val_loss: 0.7143 - val_accuracy: 0.7125\n",
      "Epoch 14/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.6645 - accuracy: 0.7333 - val_loss: 0.6541 - val_accuracy: 0.7323\n",
      "Epoch 15/200\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.6285 - accuracy: 0.7495 - val_loss: 0.6038 - val_accuracy: 0.7584\n",
      "Epoch 16/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.6493 - accuracy: 0.7470 - val_loss: 0.5659 - val_accuracy: 0.7767\n",
      "Epoch 17/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.5512 - accuracy: 0.7828 - val_loss: 0.5195 - val_accuracy: 0.7884\n",
      "Epoch 18/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.5546 - accuracy: 0.7843 - val_loss: 0.5139 - val_accuracy: 0.7994\n",
      "Epoch 19/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.5010 - accuracy: 0.8056 - val_loss: 0.5177 - val_accuracy: 0.8038\n",
      "Epoch 20/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.4811 - accuracy: 0.8165 - val_loss: 0.4666 - val_accuracy: 0.8262\n",
      "Epoch 21/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.4712 - accuracy: 0.8205 - val_loss: 0.4916 - val_accuracy: 0.8102\n",
      "Epoch 22/200\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.4610 - accuracy: 0.8239 - val_loss: 0.4580 - val_accuracy: 0.8410\n",
      "Epoch 23/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.4165 - accuracy: 0.8426 - val_loss: 0.4341 - val_accuracy: 0.8416\n",
      "Epoch 24/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.3836 - accuracy: 0.8584 - val_loss: 0.4018 - val_accuracy: 0.8471\n",
      "Epoch 25/200\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.3797 - accuracy: 0.8585 - val_loss: 0.4244 - val_accuracy: 0.8372\n",
      "Epoch 26/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.3701 - accuracy: 0.8640 - val_loss: 0.3640 - val_accuracy: 0.8648\n",
      "Epoch 27/200\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.3460 - accuracy: 0.8731 - val_loss: 0.4318 - val_accuracy: 0.8474\n",
      "Epoch 28/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.3474 - accuracy: 0.8738 - val_loss: 0.3547 - val_accuracy: 0.8730\n",
      "Epoch 29/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.3234 - accuracy: 0.8829 - val_loss: 0.3341 - val_accuracy: 0.8785\n",
      "Epoch 30/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.3208 - accuracy: 0.8824 - val_loss: 0.3401 - val_accuracy: 0.8770\n",
      "Epoch 31/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.3419 - accuracy: 0.8767 - val_loss: 0.3839 - val_accuracy: 0.8578\n",
      "Epoch 32/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.3033 - accuracy: 0.8897 - val_loss: 0.3564 - val_accuracy: 0.8672\n",
      "Epoch 33/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.2945 - accuracy: 0.8906 - val_loss: 0.3234 - val_accuracy: 0.8828\n",
      "Epoch 34/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.2922 - accuracy: 0.8945 - val_loss: 0.3266 - val_accuracy: 0.8823\n",
      "Epoch 35/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.2763 - accuracy: 0.9009 - val_loss: 0.2909 - val_accuracy: 0.8980\n",
      "Epoch 36/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.2593 - accuracy: 0.9084 - val_loss: 0.2976 - val_accuracy: 0.8971\n",
      "Epoch 37/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.5221 - accuracy: 0.8093 - val_loss: 0.3668 - val_accuracy: 0.8637\n",
      "Epoch 38/200\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.2954 - accuracy: 0.8921 - val_loss: 0.3091 - val_accuracy: 0.8904\n",
      "Epoch 39/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.2531 - accuracy: 0.9088 - val_loss: 0.2925 - val_accuracy: 0.8927\n",
      "Epoch 40/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.2563 - accuracy: 0.9093 - val_loss: 0.2569 - val_accuracy: 0.9108\n",
      "Epoch 41/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.2264 - accuracy: 0.9197 - val_loss: 0.2853 - val_accuracy: 0.9026\n",
      "Epoch 42/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.2208 - accuracy: 0.9229 - val_loss: 0.2673 - val_accuracy: 0.9090\n",
      "Epoch 43/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.2129 - accuracy: 0.9258 - val_loss: 0.2536 - val_accuracy: 0.9137\n",
      "Epoch 44/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.2354 - accuracy: 0.9184 - val_loss: 0.2925 - val_accuracy: 0.9003\n",
      "Epoch 45/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.2239 - accuracy: 0.9218 - val_loss: 0.2679 - val_accuracy: 0.9073\n",
      "Epoch 46/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.2069 - accuracy: 0.9279 - val_loss: 0.2586 - val_accuracy: 0.9134\n",
      "Epoch 47/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.2070 - accuracy: 0.9283 - val_loss: 0.2560 - val_accuracy: 0.9073\n",
      "Epoch 48/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.2019 - accuracy: 0.9295 - val_loss: 0.2589 - val_accuracy: 0.9116\n",
      "Epoch 49/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.2074 - accuracy: 0.9266 - val_loss: 0.2541 - val_accuracy: 0.9174\n",
      "Epoch 50/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.2006 - accuracy: 0.9291 - val_loss: 0.2505 - val_accuracy: 0.9192\n",
      "Epoch 51/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1873 - accuracy: 0.9360 - val_loss: 0.2442 - val_accuracy: 0.9198\n",
      "Epoch 52/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1852 - accuracy: 0.9354 - val_loss: 0.2429 - val_accuracy: 0.9154\n",
      "Epoch 53/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.2356 - accuracy: 0.9198 - val_loss: 0.2293 - val_accuracy: 0.9192\n",
      "Epoch 54/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1901 - accuracy: 0.9345 - val_loss: 0.2572 - val_accuracy: 0.9099\n",
      "Epoch 55/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1760 - accuracy: 0.9379 - val_loss: 0.2648 - val_accuracy: 0.9108\n",
      "Epoch 56/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1738 - accuracy: 0.9416 - val_loss: 0.2230 - val_accuracy: 0.9215\n",
      "Epoch 57/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1680 - accuracy: 0.9415 - val_loss: 0.2192 - val_accuracy: 0.9308\n",
      "Epoch 58/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1718 - accuracy: 0.9414 - val_loss: 0.2276 - val_accuracy: 0.9224\n",
      "Epoch 59/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1884 - accuracy: 0.9356 - val_loss: 0.2518 - val_accuracy: 0.9154\n",
      "Epoch 60/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1833 - accuracy: 0.9364 - val_loss: 0.2371 - val_accuracy: 0.9224\n",
      "Epoch 61/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1743 - accuracy: 0.9390 - val_loss: 0.2304 - val_accuracy: 0.9270\n",
      "Epoch 62/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1843 - accuracy: 0.9365 - val_loss: 0.2866 - val_accuracy: 0.9047\n",
      "Epoch 63/200\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.1685 - accuracy: 0.9441 - val_loss: 0.2109 - val_accuracy: 0.9317\n",
      "Epoch 64/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1520 - accuracy: 0.9492 - val_loss: 0.2282 - val_accuracy: 0.9206\n",
      "Epoch 65/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1618 - accuracy: 0.9434 - val_loss: 0.2940 - val_accuracy: 0.9070\n",
      "Epoch 66/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1614 - accuracy: 0.9444 - val_loss: 0.2219 - val_accuracy: 0.9288\n",
      "Epoch 67/200\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.1569 - accuracy: 0.9473 - val_loss: 0.2353 - val_accuracy: 0.9250\n",
      "Epoch 68/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1489 - accuracy: 0.9508 - val_loss: 0.2290 - val_accuracy: 0.9270\n",
      "Epoch 69/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1570 - accuracy: 0.9461 - val_loss: 0.2270 - val_accuracy: 0.9291\n",
      "Epoch 70/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1543 - accuracy: 0.9474 - val_loss: 0.2472 - val_accuracy: 0.9215\n",
      "Epoch 71/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1517 - accuracy: 0.9469 - val_loss: 0.2468 - val_accuracy: 0.9189\n",
      "Epoch 72/200\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.1595 - accuracy: 0.9447 - val_loss: 0.2119 - val_accuracy: 0.9331\n",
      "Epoch 73/200\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.1377 - accuracy: 0.9513 - val_loss: 0.2311 - val_accuracy: 0.9253\n",
      "Epoch 74/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1430 - accuracy: 0.9521 - val_loss: 0.2330 - val_accuracy: 0.9265\n",
      "Epoch 75/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1532 - accuracy: 0.9484 - val_loss: 0.2034 - val_accuracy: 0.9387\n",
      "Epoch 76/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1339 - accuracy: 0.9540 - val_loss: 0.2171 - val_accuracy: 0.9328\n",
      "Epoch 77/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1384 - accuracy: 0.9530 - val_loss: 0.2253 - val_accuracy: 0.9297\n",
      "Epoch 78/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1503 - accuracy: 0.9495 - val_loss: 0.2328 - val_accuracy: 0.9285\n",
      "Epoch 79/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1285 - accuracy: 0.9558 - val_loss: 0.2524 - val_accuracy: 0.9233\n",
      "Epoch 80/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1277 - accuracy: 0.9556 - val_loss: 0.2133 - val_accuracy: 0.9366\n",
      "Epoch 81/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1458 - accuracy: 0.9502 - val_loss: 0.2167 - val_accuracy: 0.9230\n",
      "Epoch 82/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1312 - accuracy: 0.9540 - val_loss: 0.2356 - val_accuracy: 0.9227\n",
      "Epoch 83/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1334 - accuracy: 0.9536 - val_loss: 0.2180 - val_accuracy: 0.9331\n",
      "Epoch 84/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1297 - accuracy: 0.9545 - val_loss: 0.2173 - val_accuracy: 0.9302\n",
      "Epoch 85/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1341 - accuracy: 0.9523 - val_loss: 0.2146 - val_accuracy: 0.9337\n",
      "Epoch 86/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1263 - accuracy: 0.9561 - val_loss: 0.2206 - val_accuracy: 0.9285\n",
      "Epoch 87/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1323 - accuracy: 0.9547 - val_loss: 0.2317 - val_accuracy: 0.9337\n",
      "Epoch 88/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1218 - accuracy: 0.9577 - val_loss: 0.2285 - val_accuracy: 0.9337\n",
      "Epoch 89/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1216 - accuracy: 0.9584 - val_loss: 0.2105 - val_accuracy: 0.9358\n",
      "Epoch 90/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1158 - accuracy: 0.9614 - val_loss: 0.2149 - val_accuracy: 0.9355\n",
      "Epoch 91/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1242 - accuracy: 0.9578 - val_loss: 0.1980 - val_accuracy: 0.9390\n",
      "Epoch 92/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1295 - accuracy: 0.9551 - val_loss: 0.2395 - val_accuracy: 0.9224\n",
      "Epoch 93/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1423 - accuracy: 0.9510 - val_loss: 0.1990 - val_accuracy: 0.9337\n",
      "Epoch 94/200\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.1326 - accuracy: 0.9537 - val_loss: 0.2057 - val_accuracy: 0.9384\n",
      "Epoch 95/200\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.1172 - accuracy: 0.9589 - val_loss: 0.2289 - val_accuracy: 0.9311\n",
      "Epoch 96/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1274 - accuracy: 0.9563 - val_loss: 0.2362 - val_accuracy: 0.9273\n",
      "Epoch 97/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1221 - accuracy: 0.9592 - val_loss: 0.2289 - val_accuracy: 0.9291\n",
      "Epoch 98/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1470 - accuracy: 0.9499 - val_loss: 0.2171 - val_accuracy: 0.9323\n",
      "Epoch 99/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1233 - accuracy: 0.9579 - val_loss: 0.2008 - val_accuracy: 0.9378\n",
      "Epoch 100/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1063 - accuracy: 0.9633 - val_loss: 0.2450 - val_accuracy: 0.9273\n",
      "Epoch 101/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1155 - accuracy: 0.9614 - val_loss: 0.1976 - val_accuracy: 0.9424\n",
      "Epoch 102/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1104 - accuracy: 0.9621 - val_loss: 0.1893 - val_accuracy: 0.9392\n",
      "Epoch 103/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1083 - accuracy: 0.9624 - val_loss: 0.2552 - val_accuracy: 0.9279\n",
      "Epoch 104/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1204 - accuracy: 0.9578 - val_loss: 0.2197 - val_accuracy: 0.9291\n",
      "Epoch 105/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1172 - accuracy: 0.9596 - val_loss: 0.1935 - val_accuracy: 0.9419\n",
      "Epoch 106/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1074 - accuracy: 0.9630 - val_loss: 0.2107 - val_accuracy: 0.9358\n",
      "Epoch 107/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1264 - accuracy: 0.9574 - val_loss: 0.2044 - val_accuracy: 0.9360\n",
      "Epoch 108/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1379 - accuracy: 0.9522 - val_loss: 0.2012 - val_accuracy: 0.9369\n",
      "Epoch 109/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1038 - accuracy: 0.9647 - val_loss: 0.1851 - val_accuracy: 0.9366\n",
      "Epoch 110/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1099 - accuracy: 0.9626 - val_loss: 0.2849 - val_accuracy: 0.9172\n",
      "Epoch 111/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1077 - accuracy: 0.9642 - val_loss: 0.2067 - val_accuracy: 0.9331\n",
      "Epoch 112/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1041 - accuracy: 0.9638 - val_loss: 0.1966 - val_accuracy: 0.9413\n",
      "Epoch 113/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1052 - accuracy: 0.9637 - val_loss: 0.2100 - val_accuracy: 0.9378\n",
      "Epoch 114/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1004 - accuracy: 0.9662 - val_loss: 0.2441 - val_accuracy: 0.9247\n",
      "Epoch 115/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1035 - accuracy: 0.9651 - val_loss: 0.2058 - val_accuracy: 0.9387\n",
      "Epoch 116/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1103 - accuracy: 0.9640 - val_loss: 0.2376 - val_accuracy: 0.9314\n",
      "Epoch 117/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1106 - accuracy: 0.9607 - val_loss: 0.1731 - val_accuracy: 0.9480\n",
      "Epoch 118/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.0974 - accuracy: 0.9663 - val_loss: 0.2028 - val_accuracy: 0.9375\n",
      "Epoch 119/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1084 - accuracy: 0.9640 - val_loss: 0.2121 - val_accuracy: 0.9366\n",
      "Epoch 120/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1029 - accuracy: 0.9647 - val_loss: 0.2427 - val_accuracy: 0.9331\n",
      "Epoch 121/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1078 - accuracy: 0.9618 - val_loss: 0.2239 - val_accuracy: 0.9320\n",
      "Epoch 122/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1040 - accuracy: 0.9642 - val_loss: 0.2245 - val_accuracy: 0.9259\n",
      "Epoch 123/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1207 - accuracy: 0.9582 - val_loss: 0.1801 - val_accuracy: 0.9451\n",
      "Epoch 124/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.0968 - accuracy: 0.9671 - val_loss: 0.1889 - val_accuracy: 0.9410\n",
      "Epoch 125/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1047 - accuracy: 0.9644 - val_loss: 0.2262 - val_accuracy: 0.9352\n",
      "Epoch 126/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1262 - accuracy: 0.9560 - val_loss: 0.1984 - val_accuracy: 0.9401\n",
      "Epoch 127/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.0943 - accuracy: 0.9664 - val_loss: 0.1927 - val_accuracy: 0.9404\n",
      "Epoch 128/200\n",
      "215/215 [==============================] - 5s 23ms/step - loss: 0.1080 - accuracy: 0.9633 - val_loss: 0.2188 - val_accuracy: 0.9372\n",
      "Epoch 129/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.1000 - accuracy: 0.9666 - val_loss: 0.1833 - val_accuracy: 0.9413\n",
      "Epoch 130/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.0974 - accuracy: 0.9671 - val_loss: 0.2213 - val_accuracy: 0.9297\n",
      "Epoch 131/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.0868 - accuracy: 0.9704 - val_loss: 0.2088 - val_accuracy: 0.9358\n",
      "Epoch 132/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.0936 - accuracy: 0.9678 - val_loss: 0.2280 - val_accuracy: 0.9302\n",
      "Epoch 133/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.0955 - accuracy: 0.9666 - val_loss: 0.2099 - val_accuracy: 0.9366\n",
      "Epoch 134/200\n",
      "215/215 [==============================] - 4s 21ms/step - loss: 0.1474 - accuracy: 0.9488 - val_loss: 0.2129 - val_accuracy: 0.9366\n",
      "Epoch 135/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.1014 - accuracy: 0.9656 - val_loss: 0.1836 - val_accuracy: 0.9445\n",
      "Epoch 136/200\n",
      "215/215 [==============================] - 5s 21ms/step - loss: 0.0920 - accuracy: 0.9691 - val_loss: 0.1956 - val_accuracy: 0.9445\n",
      "Epoch 137/200\n",
      "215/215 [==============================] - 5s 22ms/step - loss: 0.0994 - accuracy: 0.9658 - val_loss: 0.1968 - val_accuracy: 0.9433\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history =model.fit(X_train.astype(np.float32),y_train_onehot, epochs=200, batch_size=batch_size,\n",
    "validation_data=(X_val.astype(np.float32),y_val_onehot), callbacks=[early_stopping ])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('try_CRNN_LSTM.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
